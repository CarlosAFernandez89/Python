{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-02T22:47:51.258642600Z",
     "start_time": "2023-10-02T22:47:49.632295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert all files into .txt and name them with their \n",
    "# correct folder name since they are all just numbers.\n",
    "\n",
    "import os\n",
    "\n",
    "# Set A\n",
    "#paths = [\"ProjectTwo_Data/easy_ham\",\n",
    "#         \"ProjectTwo_Data/hard_ham\",\n",
    "#         \"ProjectTwo_Data/spam\"\n",
    "#         ]\n",
    "\n",
    "# Set B\n",
    "#paths = [\"ProjectTwo_Data_BIG/easy_ham\",\n",
    "#         \"ProjectTwo_Data_BIG/hard_ham\",\n",
    "#         \"ProjectTwo_Data_BIG/spam\"\n",
    "#         ]\n",
    "\n",
    "# Set C\n",
    "paths = [\"ProjectTwo_Data_BIG/easy_ham\",\n",
    "         \"ProjectTwo_Data_BIG/easy_ham_2\",\n",
    "         \"ProjectTwo_Data_BIG/hard_ham\",\n",
    "         \"ProjectTwo_Data_BIG/spam\",\n",
    "         \"ProjectTwo_Data_BIG/spam_2\"\n",
    "         ]\n",
    "\n",
    "for path in paths:\n",
    "    for filename in os.listdir(path):\n",
    "        src = os.path.join(path, filename)\n",
    "        dst = src + \".txt\"\n",
    "        os.rename(src, dst)\n",
    "        \n",
    "for path in paths:\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            parts = filename.split('.')\n",
    "            new_name = parts[0] + \".txt\"\n",
    "            src = os.path.join(path, filename)\n",
    "            dst = os.path.join(path, new_name)\n",
    "            os.rename(src, dst)\n",
    "\n",
    "for path in paths:\n",
    "    folder_name = os.path.basename(path)  # This will get 'easy_ham', 'hard_ham', or 'spam'\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        new_name = f\"{folder_name}_{filename}\"\n",
    "        src = os.path.join(path, filename)\n",
    "        dst = os.path.join(path, new_name)\n",
    "\n",
    "        os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Move every 4th file into a \"testing\" folder\n",
    "import shutil\n",
    "\n",
    "for path in paths:\n",
    "    test_path = os.path.join(path, \"testing\")\n",
    "    if not os.path.exists(test_path):\n",
    "        os.makedirs(test_path)\n",
    "\n",
    "\n",
    "for path in paths:\n",
    "    files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    for index, filename in enumerate(files, start=1):\n",
    "        if index % 4 == 0:\n",
    "            src = os.path.join(path, filename)\n",
    "            dst = os.path.join(path, \"testing\", filename)\n",
    "            shutil.move(src, dst)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T22:47:54.867983800Z",
     "start_time": "2023-10-02T22:47:52.712470900Z"
    }
   },
   "id": "7e13eb9c430ecd47"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cfernandezmq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define a function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatization and Stemming\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T22:47:56.689722400Z",
     "start_time": "2023-10-02T22:47:55.882504800Z"
    }
   },
   "id": "ea3cf6748bdf5336"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391310\n",
      "P(spam|X=free rate money adv mortgag): 0.999999386867987\n",
      "P(spam|X=free rate money adv ilug): 0.9997380074183535\n",
      "P(spam|X=free rate money razoru ilug): 0.9803554328781104\n",
      "P(spam|X=free rate wa razoru ilug): 0.1751626651956688\n",
      "P(spam|X=free spambay wa razoru ilug): 0.0007402432146838626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Lists to store subject lines and their labels\n",
    "subject_lines = []\n",
    "labels = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for path in paths:\n",
    "    for filename in os.listdir(path):\n",
    "        full_path = os.path.join(path, filename)\n",
    "        if os.path.isfile(full_path):\n",
    "            with open(full_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    count += 1\n",
    "                    if line.startswith(\"Subject:\"):\n",
    "                        # Apply preprocessing here\n",
    "                        cleaned_subject = preprocess_text(line[len(\"Subject:\"):].strip())\n",
    "                        subject_lines.append(cleaned_subject)\n",
    "                        labels.append('spam' if \"spam\" in path else 'ham')\n",
    "\n",
    "print(count)\n",
    "# Vectorize the subject lines\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]+\\b', stop_words='english')\n",
    "X = vectorizer.fit_transform(subject_lines)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB(alpha=1)  # Setting smoothing parameter alpha = 1\n",
    "clf.fit(X, labels)\n",
    "\n",
    "# To predict the probability P(spam|X=a) for a new subject line\n",
    "subject_line_a = [\n",
    "    preprocess_text(\"free rate money adv mortgag\"),\n",
    "    preprocess_text(\"free rate money adv ilug\"),\n",
    "    preprocess_text(\"free rate money razorus ilug\"),\n",
    "    preprocess_text(\"free rate wa razorus ilug\"),\n",
    "    preprocess_text(\"free spambay wa razorus ilug\")\n",
    "]\n",
    "\n",
    "for subject in subject_line_a:\n",
    "    X_a = vectorizer.transform([subject])  # Note that the input should be in list form\n",
    "    probabilities = clf.predict_proba(X_a)\n",
    "    p_spam_given_a = probabilities[0][clf.classes_ == 'spam'][0]\n",
    "\n",
    "    print(f\"P(spam|X={subject}): {p_spam_given_a}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T22:48:00.898442200Z",
     "start_time": "2023-10-02T22:47:59.204781800Z"
    }
   },
   "id": "af79a360c7e3bc65"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract log probabilities of features given a class\n",
    "log_prob_wk_spam = clf.feature_log_prob_[clf.classes_ == 'spam'][0]\n",
    "log_prob_wk_ham = clf.feature_log_prob_[clf.classes_ == 'ham'][0]\n",
    "\n",
    "# Convert log probabilities back to probabilities\n",
    "prob_wk_spam = np.exp(log_prob_wk_spam)\n",
    "prob_wk_ham = np.exp(log_prob_wk_ham)\n",
    "\n",
    "# Get top 5 indices for spam and ham\n",
    "top5_spam_indices = prob_wk_spam.argsort()[-5:][::-1]\n",
    "top5_ham_indices = prob_wk_ham.argsort()[-5:][::-1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T21:56:32.570180Z",
     "start_time": "2023-10-02T21:56:32.564554200Z"
    }
   },
   "id": "f5d1659d0c3090a1"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words with highest P(spam|wk):\n",
      "free\n",
      "adv\n",
      "rate\n",
      "money\n",
      "mortgag\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Output top 5 words with highest probabilities for spam and ham\n",
    "print(\"Top 5 words with highest P(spam|wk):\")\n",
    "for i in top5_spam_indices:\n",
    "    print(vectorizer.get_feature_names_out()[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T21:56:34.280481Z",
     "start_time": "2023-10-02T21:56:34.273082500Z"
    }
   },
   "id": "ae4c9e1b794fd1cf"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 words with highest P(ham|wk):\n",
      "ilug\n",
      "razorus\n",
      "satalk\n",
      "wa\n",
      "spambay\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 5 words with highest P(ham|wk):\")\n",
    "for i in top5_ham_indices:\n",
    "    print(vectorizer.get_feature_names_out()[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T21:56:35.317880900Z",
     "start_time": "2023-10-02T21:56:35.315559Z"
    }
   },
   "id": "af391faeac518ff8"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Rate: 0.844205193160228\n",
      "Precision Rate: 0.7970297029702971\n",
      "Recall Rate: 0.6625514403292181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# List of paths for test data\n",
    "paths_test = [os.path.join(p, 'testing') for p in paths]\n",
    "\n",
    "# Prepare the test data\n",
    "test_ham_subject_lines = []\n",
    "test_spam_subject_lines = []\n",
    "\n",
    "# Assuming paths_test is a list of testing paths\n",
    "for path in paths_test:\n",
    "    for filename in os.listdir(path):\n",
    "        full_path = os.path.join(path, filename)\n",
    "        if os.path.isfile(full_path):\n",
    "            with open(full_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    if line.startswith(\"Subject:\"):\n",
    "                        if \"spam\" in path:\n",
    "                            test_spam_subject_lines.append(line[len(\"Subject:\"):].strip())\n",
    "                        else:\n",
    "                            test_ham_subject_lines.append(line[len(\"Subject:\"):].strip())\n",
    "\n",
    "# Create a combined list and true labels\n",
    "test_subject_lines = test_ham_subject_lines + test_spam_subject_lines\n",
    "true_labels = ['ham'] * len(test_ham_subject_lines) + ['spam'] * len(test_spam_subject_lines)\n",
    "\n",
    "# Vectorize the test data\n",
    "X_test = vectorizer.transform(test_subject_lines)\n",
    "\n",
    "# Predict using the trained classifier\n",
    "predicted_labels = clf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, pos_label='spam')\n",
    "recall = recall_score(true_labels, predicted_labels, pos_label='spam')\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy Rate: {accuracy}\")\n",
    "print(f\"Precision Rate: {precision}\")\n",
    "print(f\"Recall Rate: {recall}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T21:56:36.835072Z",
     "start_time": "2023-10-02T21:56:36.663520Z"
    }
   },
   "id": "f83668d91d4db99a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
